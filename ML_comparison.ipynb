{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ===== Load data; replace with your paths =====\n",
    "data_dir = './datasets/'\n",
    "result_dir = './results/'\n",
    "data_cleaned = pd.read_pickle(data_dir+'data_cleaned.pkl')\n",
    "descriptors = pd.read_csv(data_dir+'Jarvis_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_l_idx = np.loadtxt(result_dir + 'data_l.csv', dtype='int', delimiter=',')\n",
    "data_l = data_cleaned.loc[data_l_idx]  # Labeled dataset\n",
    "data_test_idx = np.loadtxt(result_dir + 'data_test.csv', dtype='int', delimiter=',')\n",
    "data_test = data_cleaned.loc[data_test_idx]  # Test set\n",
    "data_u = data_cleaned.drop(index=np.union1d(data_l_idx, data_test_idx), inplace=False)  # Unlabeled dataset\n",
    "\n",
    "sample_path = np.loadtxt(result_dir + 'sample_path.csv', dtype='int', delimiter=',')\n",
    "samples = sample_path[sample_path != 0]\n",
    "samples_rand = data_u.sample(n=samples.shape[0], random_state=42)  # Randomly selected samples\n",
    "\n",
    "data_l_etal_idx = np.concatenate((data_l_idx, samples)) # Two training sets\n",
    "data_l_rand = pd.concat([data_l, samples_rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize training and test sets\n",
    "x_etal= descriptors.loc[data_l_etal_idx]\n",
    "x_rand = descriptors.loc[data_l_rand.index]\n",
    "x_test = descriptors.loc[data_test_idx]\n",
    "\n",
    "# Responses: bulk and shear moduli\n",
    "B_etal = data_cleaned.loc[data_l_etal_idx]['bulk_modulus_kv']\n",
    "B_rand = data_cleaned.loc[data_l_rand.index]['bulk_modulus_kv']\n",
    "B_test = data_cleaned.loc[data_test_idx]['bulk_modulus_kv']\n",
    "\n",
    "G_etal = data_cleaned.loc[data_l_etal_idx]['shear_modulus_gv']\n",
    "G_rand = data_cleaned.loc[data_l_rand.index]['shear_modulus_gv']\n",
    "G_test = data_cleaned.loc[data_test_idx]['shear_modulus_gv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "A support vector machine (SVM)-based ML pipeline.\n",
    "\n",
    "The material feature vector $x$ is first standardized, then reduced dimension by principal component analysis (PCA). The first 7 principal components are taken as the representation and input to an SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_rep = 10\n",
    "def train_svr_etal_B(i):\n",
    "    svr_etal_B = make_pipeline(StandardScaler(), PCA(n_components=7), SVR(C=10, kernel='rbf'))\n",
    "    svr_etal_B.fit(x_etal, B_etal)\n",
    "    return svr_etal_B.score(x_test, B_test)\n",
    "\n",
    "def train_svr_rand_B(i):\n",
    "    svr_rand_B = make_pipeline(StandardScaler(), PCA(n_components=7), SVR(C=10, kernel='rbf'))\n",
    "    svr_rand_B.fit(x_rand, B_rand)\n",
    "    return svr_rand_B.score(x_test, B_test)\n",
    "\n",
    "svr_r2_etal_B = Parallel(n_jobs=10)(delayed(train_svr_etal_B)(i) for i in range (n_rep))\n",
    "svr_r2_rand_B = Parallel(n_jobs=10)(delayed(train_svr_rand_B)(i) for i in range (n_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid cross-validation (CV) search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "svr = Pipeline([('scaler', StandardScaler()), ('pca', PCA()), ('svr', SVR())])\n",
    "parameters = {'svr__kernel':('rbf', 'poly', 'sigmoid'), 'svr__C': [1,5,10], 'pca__n_components': [3,5,7,9]}\n",
    "clf = GridSearchCV(svr, parameters)\n",
    "clf.fit(x_etal, B_etal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "The adaptive boosting model under default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "n_rep = 30\n",
    "ab_r2_etal_G = np.zeros(n_rep)\n",
    "ab_r2_rand_G = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    ab_etal_G = AdaBoostRegressor(random_state=i).fit(x_etal, G_etal)\n",
    "    ab_rand_G = AdaBoostRegressor(random_state=i).fit(x_rand, G_rand)\n",
    "    ab_r2_rand_G[i] = ab_rand_G.score(x_test, G_test)\n",
    "    ab_r2_etal_G[i] = ab_etal_G.score(x_test, G_test)\n",
    "\n",
    "ab_r2_etal_B = np.zeros(n_rep)\n",
    "ab_r2_rand_B = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    ab_etal_B = AdaBoostRegressor(random_state=i).fit(x_etal, B_etal)\n",
    "    ab_rand_B = AdaBoostRegressor(random_state=i).fit(x_rand, B_rand)\n",
    "    ab_r2_rand_B[i] = ab_rand_B.score(x_test, B_test)\n",
    "    ab_r2_etal_B[i] = ab_etal_B.score(x_test, B_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Tune random forest hyperparameters using grid CV search. Effects of `min_samples_leaf` and `min_samples_split` are found to be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "parameters = {'bootstrap': [True, False],\n",
    "              'max_depth': [30, 100, None],\n",
    "              'max_features': ['log2', 'sqrt', None],\n",
    "              'n_estimators': [30, 100, 300],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10]\n",
    "             }\n",
    "rf = RandomForestRegressor()\n",
    "rf_cv = GridSearchCV(rf, parameters, verbose=3, n_jobs=-1)\n",
    "# For other dataset/response, Change G to B; rand to etal\n",
    "rf_cv.fit(x_rand, G_rand)\n",
    "rf_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import Parallel, delayed\n",
    "n_rep = 30\n",
    "rf_r2_etal_G = np.zeros(n_rep)\n",
    "rf_r2_rand_G = np.zeros(n_rep)\n",
    "def train_rf_etal_G(i):\n",
    "    rf_etal_G = RandomForestRegressor(random_state=i, max_depth=30, n_estimators=300).fit(x_etal, G_etal)\n",
    "    return rf_etal_G.score(x_test, G_test)\n",
    "\n",
    "def train_rf_rand_G(i):\n",
    "    rf_rand_G = RandomForestRegressor(random_state=i, max_depth=30, n_estimators=300).fit(x_rand, G_rand)\n",
    "    return rf_rand_G.score(x_test, G_test)\n",
    "\n",
    "rf_r2_etal_G = Parallel(n_jobs=10)(delayed(train_rf_etal_G)(i) for i in range (n_rep))\n",
    "rf_r2_rand_G = Parallel(n_jobs=10)(delayed(train_rf_rand_G)(i) for i in range (n_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r2_etal_B = np.zeros(n_rep)\n",
    "rf_r2_rand_B = np.zeros(n_rep)\n",
    "\n",
    "def train_rf_etal_B(i):\n",
    "    rf_etal_B = RandomForestRegressor(random_state=i, max_depth=100, n_estimators=300).fit(x_etal, B_etal)\n",
    "    return rf_etal_B.score(x_test, B_test)\n",
    "    \n",
    "    \n",
    "def train_rf_rand_B(i):\n",
    "    rf_rand_B = RandomForestRegressor(random_state=i, max_depth=100, n_estimators=300).fit(x_rand, B_rand)\n",
    "    return rf_rand_B.score(x_test, B_test)\n",
    "\n",
    "rf_r2_etal_B = Parallel(n_jobs=10)(delayed(train_rf_etal_B)(i) for i in range (n_rep))\n",
    "rf_r2_rand_B = Parallel(n_jobs=10)(delayed(train_rf_rand_B)(i) for i in range (n_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most improved samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_etal_B = RandomForestRegressor(random_state=42, max_depth=30, n_estimators=300).fit(x_etal, B_etal)\n",
    "rf_etal_G = RandomForestRegressor(random_state=42, max_depth=30, n_estimators=300).fit(x_etal, G_etal)\n",
    "rf_rand_B = RandomForestRegressor(random_state=42, max_depth=100, n_estimators=300).fit(x_rand, B_rand)\n",
    "rf_rand_G = RandomForestRegressor(random_state=42, max_depth=100, n_estimators=300).fit(x_rand, G_rand)\n",
    "\n",
    "improv = np.abs(rf_rand_G.predict(x_test) - G_test) - np.abs(rf_etal_G.predict(x_test) - G_test)\n",
    "rf_G_most_improv = improv.astype('float').nlargest(30).index\n",
    "rf_G_most_improv\n",
    "\n",
    "improv = np.abs(rf_rand_B.predict(x_test) - B_test) - np.abs(rf_etal_B.predict(x_test) - B_test)\n",
    "rf_B_most_improv = improv.astype('float').nlargest(30).index\n",
    "rf_B_most_improv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "Tune gradient boost hyperparameters using grid CV search. Again, effects of `min_samples_leaf` and `min_samples_split` are found to be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "parameters = {'learning_rate': [0.03, 0.1, 0.3],\n",
    "              'max_depth': [3, 5, 10, None],\n",
    "              'max_features': ['log2', 'sqrt', None],\n",
    "              'n_estimators': [30, 100, 300],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 3, 5]\n",
    "             }\n",
    "gb = GradientBoostingRegressor()\n",
    "gb_cv = GridSearchCV(gb, parameters, verbose=3, n_jobs=-1)\n",
    "gb_cv.fit(x_etal, B_etal)\n",
    "gb_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "n_rep = 30\n",
    "\n",
    "def train_gb_etal_G(i):\n",
    "    gb_etal_G = GradientBoostingRegressor(random_state=i, max_depth=5).fit(x_etal, G_etal)\n",
    "    return gb_etal_G.score(x_test, G_test)\n",
    "\n",
    "def train_gb_rand_G(i):\n",
    "    gb_rand_G = GradientBoostingRegressor(random_state=i, max_depth=5).fit(x_rand, G_rand)\n",
    "    return gb_rand_G.score(x_test, G_test)\n",
    "\n",
    "gb_r2_etal_G = Parallel(n_jobs=10)(delayed(train_gb_etal_G)(i) for i in range (n_rep))\n",
    "gb_r2_rand_G = Parallel(n_jobs=10)(delayed(train_gb_rand_G)(i) for i in range (n_rep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb_etal_B(i):\n",
    "    gb_etal_B = GradientBoostingRegressor(random_state=i, max_depth=5).fit(x_etal, B_etal)\n",
    "    return gb_etal_B.score(x_test, B_test)\n",
    "\n",
    "def train_gb_rand_B(i):\n",
    "    gb_rand_B = GradientBoostingRegressor(random_state=i, max_depth=5).fit(x_rand, B_rand)\n",
    "    return gb_rand_B.score(x_test, B_test)\n",
    "\n",
    "gb_r2_etal_B = Parallel(n_jobs=10)(delayed(train_gb_etal_B)(i) for i in range (n_rep))\n",
    "gb_r2_rand_B = Parallel(n_jobs=10)(delayed(train_gb_rand_B)(i) for i in range (n_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most improved samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_etal_B = GradientBoostingRegressor(random_state=42, max_depth=5).fit(x_etal, B_etal)\n",
    "gb_etal_G = GradientBoostingRegressor(random_state=42, max_depth=5).fit(x_etal, G_etal)\n",
    "gb_rand_B = GradientBoostingRegressor(random_state=42, max_depth=5).fit(x_rand, B_rand)\n",
    "gb_rand_G = GradientBoostingRegressor(random_state=42, max_depth=5).fit(x_rand, G_rand)\n",
    "\n",
    "\n",
    "improv = np.abs(gb_rand_B.predict(x_test) - B_test) - np.abs(gb_etal_B.predict(x_test) - B_test)\n",
    "gb_B_most_improv = improv.astype('float').nlargest(30).index\n",
    "gb_B_most_improv\n",
    "\n",
    "improv = np.abs(gb_rand_G.predict(x_test) - G_test) - np.abs(gb_etal_G.predict(x_test) - G_test)\n",
    "gb_G_most_improv = improv.astype('float').nlargest(30).index\n",
    "gb_G_most_improv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net\n",
    "A multi-layer perceptron (MLP) model with standardized materials features as input. Hyperparameters are tuned using Grid CV search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "n_rep = 30\n",
    "hidden_size = (128,)\n",
    "batch_size = 32\n",
    "max_iter = 500\n",
    "alpha = 0.1\n",
    "# learning_rate_init = 0.001\n",
    "activation = 'relu'\n",
    "\n",
    "def train_nn_etal_B(i):\n",
    "    nn_etal_B = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=hidden_size, random_state=i, early_stopping=True, alpha=alpha, batch_size=batch_size,\n",
    "    max_iter=max_iter, activation=activation))\n",
    "    nn_etal_B.fit(x_etal, B_etal)\n",
    "    return nn_etal_B.score(x_test, B_test)\n",
    "\n",
    "def train_nn_rand_B(i):\n",
    "    nn_rand_B = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=hidden_size, random_state=i, early_stopping=True, alpha=alpha, batch_size=batch_size,\n",
    "    max_iter=max_iter, activation=activation))\n",
    "    nn_rand_B.fit(x_rand, B_rand)\n",
    "    return nn_rand_B.score(x_test, B_test)\n",
    "\n",
    "nn_r2_etal_B = Parallel(n_jobs=10)(delayed(train_nn_etal_B)(i) for i in range (n_rep))\n",
    "nn_r2_rand_B = Parallel(n_jobs=10)(delayed(train_nn_rand_B)(i) for i in range (n_rep))\n",
    "\n",
    "\n",
    "def train_nn_etal_G(i):\n",
    "    nn_etal_G = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=hidden_size, random_state=i, early_stopping=True, alpha=alpha, batch_size=batch_size,\n",
    "    max_iter=max_iter, activation=activation))\n",
    "    nn_etal_G.fit(x_etal, G_etal)\n",
    "    return nn_etal_G.score(x_test, G_test)\n",
    "\n",
    "def train_nn_rand_G(i):\n",
    "    nn_rand_G = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=hidden_size, random_state=i, early_stopping=True, alpha=alpha, batch_size=batch_size,\n",
    "    max_iter=max_iter, activation=activation))\n",
    "    nn_rand_G.fit(x_rand, G_rand)\n",
    "    return nn_rand_G.score(x_test, G_test)\n",
    "\n",
    "nn_r2_etal_G = Parallel(n_jobs=10)(delayed(train_nn_etal_G)(i) for i in range (n_rep))\n",
    "nn_r2_rand_G = Parallel(n_jobs=10)(delayed(train_nn_rand_G)(i) for i in range (n_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid CV search for NN hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "nn = Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(early_stopping=True, max_iter=500, batch_size=32))])\n",
    "parameters = {'mlp__alpha': [0.01, 0.05, 0.1],\n",
    "              'mlp__activation': ('tanh', 'relu'),\n",
    "              'mlp__hidden_layer_sizes': ((128,), (64,64), (128,128))\n",
    "             }\n",
    "clf = GridSearchCV(nn, parameters, verbose=3, n_jobs=-1)\n",
    "clf.fit(x_etal, B_etal)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([pd.DataFrame({'r2': gb_r2_etal_B, 'model': 'gb-B', 'target': 'B', 'dataset':'etal'}),\n",
    "                     pd.DataFrame({'r2': gb_r2_etal_G, 'model': 'gb-G', 'target': 'G', 'dataset':'etal'}),\n",
    "                     pd.DataFrame({'r2': rf_r2_etal_B, 'model': 'rf-B', 'target': 'B', 'dataset':'etal'}),\n",
    "                     pd.DataFrame({'r2': rf_r2_etal_G, 'model': 'rf-G', 'target': 'G', 'dataset':'etal'}),\n",
    "                     pd.DataFrame({'r2': gb_r2_rand_B, 'model': 'gb-B', 'target': 'B', 'dataset':'rand'}),\n",
    "                     pd.DataFrame({'r2': gb_r2_rand_G, 'model': 'gb-G', 'target': 'G', 'dataset':'rand'}),\n",
    "                     pd.DataFrame({'r2': rf_r2_rand_B, 'model': 'rf-B', 'target': 'B', 'dataset':'rand'}),\n",
    "                     pd.DataFrame({'r2': rf_r2_rand_G, 'model': 'rf-G', 'target': 'G', 'dataset':'rand'}),\n",
    "                    ], ignore_index=True)\n",
    "results.to_csv('./results/ML_r2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('./results/ML_r2.csv', index_col=0)\n",
    "results.loc[results['model'] == 'gb-B', 'model'] = 'Gradient Boosting'\n",
    "results.loc[results['model'] == 'gb-G', 'model'] = 'Gradient Boosting'\n",
    "results.loc[results['model'] == 'rf-B', 'model'] = 'Random Forest'\n",
    "results.loc[results['model'] == 'rf-G', 'model'] = 'Random Forest'\n",
    "results.loc[results['dataset'] == 'etal', 'dataset'] = 'ETAL'\n",
    "results.loc[results['dataset'] == 'rand', 'dataset'] = 'RAND'\n",
    "results.columns = ['r2', 'Model', 'Target', 'Dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# results = pd.read_csv('./results/ML_r2.csv', index_col=0)\n",
    "sns.set_theme(style=\"ticks\", palette='pastel', font_scale=1.5)\n",
    "sns.boxplot(x='Model', y='r2', hue='Dataset', data=results[(results['Target'] == 'B')])\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Bulk Modulus')\n",
    "# plt.savefig('./results/r2_B.svg', bbox_inches='tight')\n",
    "plt.savefig('./results/r2_B.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", palette='pastel', font_scale=1.5)\n",
    "sns.boxplot(x='Model', y='r2', hue='Dataset', data=results[(results['Target'] == 'G')])\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Shear Modulus')\n",
    "# plt.savefig('./results/r2_G.svg', bbox_inches='tight')\n",
    "plt.savefig('./results/r2_G.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f66a74a78460c2692b274b48ce17214f5eb55d57c3113b770dcb94e4d958ddf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
